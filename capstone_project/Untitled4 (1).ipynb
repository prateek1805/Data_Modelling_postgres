{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1201c962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark\\\\spark3'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd00dfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d6def9c",
   "metadata": {},
   "outputs": [],
   "source": [
    " spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8a620af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import col,lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b946b21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,LongType,IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30fa8344",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(r\"C:\\Users\\goelp\\OneDrive\\Desktop\\archive\\bank.csv\",header = True,inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31a0a5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+------+----------+------------+------------+------------+-------+\n",
      "|Account_No|     DATE|       TRANS_DETAILS|CHQ_NO|VALUE_DATE|WITHDRAW_AMT| DEPOSIT_AMT| BALANCE_AMT|Tras_id|\n",
      "+----------+---------+--------------------+------+----------+------------+------------+------------+-------+\n",
      "|4998365300|29-Jun-17|TRF FROM  Indiafo...|  null| 29-Jun-17|        null|10,00,000.00|10,00,000.00|      1|\n",
      "|4998365301|05-Jul-17|TRF FROM  Indiafo...|  null| 05-Jul-17|        null|10,00,000.00|20,00,000.00|      2|\n",
      "|4998365302|18-Jul-17|FDRL/INTERNAL FUN...|  null| 18-Jul-17|        null| 5,00,000.00|25,00,000.00|      3|\n",
      "|4998365303|01-Aug-17|TRF FRM  Indiafor...|  null| 01-Aug-17|        null|30,00,000.00|55,00,000.00|      4|\n",
      "|4998365304|16-Aug-17|FDRL/INTERNAL FUN...|  null| 16-Aug-17|        null| 5,00,000.00|60,00,000.00|      5|\n",
      "+----------+---------+--------------------+------+----------+------------+------------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91e28bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Account_No: long (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- TRANS_DETAILS: string (nullable = true)\n",
      " |-- CHQ_NO: integer (nullable = true)\n",
      " |-- VALUE_DATE: string (nullable = true)\n",
      " |-- WITHDRAW_AMT: string (nullable = true)\n",
      " |-- DEPOSIT_AMT: string (nullable = true)\n",
      " |-- BALANCE_AMT: string (nullable = true)\n",
      " |-- Tras_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "501e4ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.filter(df.Account_No == \"4998365300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f188695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+------+----------+--------------+------------+------------------+-------+\n",
      "|Account_No|     DATE|       TRANS_DETAILS|CHQ_NO|VALUE_DATE|  WITHDRAW_AMT| DEPOSIT_AMT|       BALANCE_AMT|Tras_id|\n",
      "+----------+---------+--------------------+------+----------+--------------+------------+------------------+-------+\n",
      "|4998365300|29-Jun-17|TRF FROM  Indiafo...|  null| 29-Jun-17|          null|10,00,000.00|      10,00,000.00|      1|\n",
      "|4998365300|25-Sep-18|Indiaforensic RUP...|  null| 25-Sep-18|         28.22|        null|  -54,37,98,937.36|  10001|\n",
      "|4998365300|19-Jan-18|AEPS ACQUIRING AD...|  null| 19-Jan-18|         14.75|        null|  -53,63,87,887.14|  20001|\n",
      "|4998365300|16-Oct-15|INTERNAL FUND TRA...|  null| 16-Oct-15|1,20,00,000.00|        null|-1,45,71,54,949.93|  30001|\n",
      "|4998365300|28-Jul-15|INTERNAL FUND TRA...|  null| 28-Jul-15|  30,00,000.00|        null|-1,58,65,35,962.60|  40001|\n",
      "|4998365300|10-May-16|RTGS/YESBH1613189...|  null| 10-May-16|1,50,00,000.00|        null|-1,68,99,57,280.72|  50001|\n",
      "|4998365300|28-Jan-17|          9999922345|  null| 28-Jan-17|          null|   30,000.00|-1,68,97,68,839.66|  60001|\n",
      "|4998365300|11-Jan-18|FDRL/NATIONAL ELE...|  null| 11-Jan-18|     22,857.00|        null|-1,67,62,64,819.26|  70001|\n",
      "|4998365300|29-Aug-18|FDRL/REAL TIME GR...|  null| 29-Aug-18|1,00,00,000.00|        null|-1,68,53,31,858.06|  80001|\n",
      "|4998365300|10-Dec-15|BSES RAJDHANI POW...|  null| 10-Dec-15|     16,600.00|        null|-1,66,73,81,169.84|  90001|\n",
      "|4998365300|12-Dec-16| IMPS OW 12122016 2C|  null| 12-Dec-16|  52,00,900.86|        null|-1,85,36,97,395.56| 100001|\n",
      "|4998365300|25-Dec-17|Sweep Trf To: 409...|  null| 25-Dec-17|      9,501.00|        null|-1,88,60,60,911.49| 110001|\n",
      "|4998365300|09-Jun-17|TRF TO  Indiafore...|  null| 09-Jun-17|  34,00,000.00|        null|  -54,78,56,392.15| 120001|\n",
      "|4998365300|09-Aug-16|Indiaforensic AEP...|  null| 09-Aug-16|     72,510.00|        null|  -54,52,42,788.15| 130001|\n",
      "|4998365300|27-Aug-18|AEPS PAY Indiafor...|  null| 27-Aug-18|          null|      598.73|  -52,84,88,656.01| 140001|\n",
      "|4998365300|16-May-16|NEFT/FDRL38715964...|  null| 16-May-16|          null|30,00,000.00|-1,69,56,33,052.98| 150001|\n",
      "|4998365300|19-Dec-15|INTERNAL FUND TRA...|  null| 19-Dec-15|1,50,00,000.00|        null|-1,76,94,16,564.94| 160001|\n",
      "|4998365300|25-Jul-16|INTERNAL FUND TRA...|  null| 25-Jul-16|1,00,00,000.00|        null|-1,63,71,80,810.17| 170001|\n",
      "|4998365300|09-Jun-17|FDRL/INTERNAL FUN...|  null| 09-Jun-17|1,50,00,000.00|        null|-1,67,44,54,156.19| 180001|\n",
      "|4998365300|23-Apr-18|FDRL/REAL TIME GR...|  null| 23-Apr-18|  20,00,000.00|        null|-1,68,87,19,301.80| 190001|\n",
      "+----------+---------+--------------------+------+----------+--------------+------------+------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7abdd957",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.na.fill(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6e823b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+------+----------+------------+------------+------------+-------+\n",
      "|Account_No|     DATE|       TRANS_DETAILS|CHQ_NO|VALUE_DATE|WITHDRAW_AMT| DEPOSIT_AMT| BALANCE_AMT|Tras_id|\n",
      "+----------+---------+--------------------+------+----------+------------+------------+------------+-------+\n",
      "|4998365300|29-Jun-17|TRF FROM  Indiafo...|  null| 29-Jun-17|            |10,00,000.00|10,00,000.00|      1|\n",
      "|4998365301|05-Jul-17|TRF FROM  Indiafo...|  null| 05-Jul-17|            |10,00,000.00|20,00,000.00|      2|\n",
      "|4998365302|18-Jul-17|FDRL/INTERNAL FUN...|  null| 18-Jul-17|            | 5,00,000.00|25,00,000.00|      3|\n",
      "|4998365303|01-Aug-17|TRF FRM  Indiafor...|  null| 01-Aug-17|            |30,00,000.00|55,00,000.00|      4|\n",
      "|4998365304|16-Aug-17|FDRL/INTERNAL FUN...|  null| 16-Aug-17|            | 5,00,000.00|60,00,000.00|      5|\n",
      "|4998365305|16-Aug-17|FDRL/INTERNAL FUN...|  null| 16-Aug-17|            | 5,00,000.00|65,00,000.00|      6|\n",
      "|4998365306|16-Aug-17|FDRL/INTERNAL FUN...|  null| 16-Aug-17|            | 5,00,000.00|70,00,000.00|      7|\n",
      "|4998365307|16-Aug-17|FDRL/INTERNAL FUN...|  null| 16-Aug-17|            | 5,00,000.00|75,00,000.00|      8|\n",
      "|4998365308|16-Aug-17|FDRL/INTERNAL FUN...|  null| 16-Aug-17|            | 5,00,000.00|80,00,000.00|      9|\n",
      "|4998365309|16-Aug-17|FDRL/INTERNAL FUN...|  null| 16-Aug-17|            | 5,00,000.00|85,00,000.00|     10|\n",
      "|4998365310|16-Aug-17|INDO GIBL Indiafo...|  null| 16-Aug-17| 1,33,900.00|            |83,66,100.00|     11|\n",
      "|4998365311|16-Aug-17|INDO GIBL Indiafo...|  null| 16-Aug-17|   18,000.00|            |83,48,100.00|     12|\n",
      "|4998365312|16-Aug-17|INDO GIBL Indiafo...|  null| 16-Aug-17|    5,000.00|            |83,43,100.00|     13|\n",
      "|4998365313|16-Aug-17|INDO GIBL Indiafo...|  null| 16-Aug-17| 1,95,800.00|            |81,47,300.00|     14|\n",
      "|4998365314|16-Aug-17|INDO GIBL Indiafo...|  null| 16-Aug-17|   81,600.00|            |80,65,700.00|     15|\n",
      "|4998365315|16-Aug-17|INDO GIBL Indiafo...|  null| 16-Aug-17|   41,800.00|            |80,23,900.00|     16|\n",
      "|4998365316|16-Aug-17|INDO GIBL Indiafo...|  null| 16-Aug-17|   98,500.00|            |79,25,400.00|     17|\n",
      "|4998365317|16-Aug-17|INDO GIBL Indiafo...|  null| 16-Aug-17| 1,43,800.00|            |77,81,600.00|     18|\n",
      "|4998365318|16-Aug-17|INDO GIBL Indiafo...|  null| 16-Aug-17| 3,31,650.00|            |74,49,950.00|     19|\n",
      "|4998365319|16-Aug-17|INDO GIBL Indiafo...|  null| 16-Aug-17| 1,29,000.00|            |73,20,950.00|     20|\n",
      "+----------+---------+--------------------+------+----------+------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b90b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bank_log_details = df2.select(\"Account_No\",\n",
    "                              \"DATE\",\n",
    "                              \"TRANS_DETAILS\",\n",
    "                              \"CHQ_NO\",\n",
    "                              \"VALUE_DATE\",\n",
    "                              \"WITHDRAW_AMT\",\n",
    "                              \"DEPOSIT_AMT\",\n",
    "                              \"BALANCE_AMT\",\n",
    "                              \"Tras_id\").drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19aec006",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bank_log_details = Bank_log_details.withColumn(\"Action\", \\\n",
    "   when((Bank_log_details.WITHDRAW_AMT !=\"\"), lit(\"Withdraw\"))\\\n",
    "     .otherwise(lit(\"Deposit\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea6ff3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+------+----------+------------+------------+----------------+-------+--------+\n",
      "|Account_No|     DATE|       TRANS_DETAILS|CHQ_NO|VALUE_DATE|WITHDRAW_AMT| DEPOSIT_AMT|     BALANCE_AMT|Tras_id|  Action|\n",
      "+----------+---------+--------------------+------+----------+------------+------------+----------------+-------+--------+\n",
      "|4998365327|16-Aug-17|INDO GIBL Indiafo...|  null| 16-Aug-17|   78,100.00|            |    62,01,467.00|     28|Withdraw|\n",
      "|4998365561|07-Feb-18|INDO GIBL Indiafo...|  null| 07-Feb-18|   61,500.00|            |    14,35,695.00|    262|Withdraw|\n",
      "|4998366033|29-Sep-18|Indfor INCOME IND...|  null| 29-Sep-18|    1,920.00|            |    10,28,562.00|    734|Withdraw|\n",
      "|4998366937|01-Mar-18|FDRL/INTERNAL FUN...|  null| 01-Mar-18|            | 3,00,000.00|    11,04,941.41|   1638| Deposit|\n",
      "|4998366986|09-Apr-18|FDRL/INTERNAL FUN...|  null| 09-Apr-18|            | 3,00,000.00|     8,80,613.78|   1687| Deposit|\n",
      "|4998367070|05-Jun-18|    BBPS DT 05062018|  null| 05-Jun-18|   78,982.56|            |     8,81,446.90|   1771|Withdraw|\n",
      "|4998367242|02-Nov-18|    BBPS DT 02112018|  null| 02-Nov-18|            | 3,55,637.06|    14,92,201.77|   1943| Deposit|\n",
      "|4998367549|30-Jul-16|NEFT/SBIN11621243...|  null| 30-Jul-16|            |    5,000.00|     6,47,578.60|   2250| Deposit|\n",
      "|4998367676|26-Sep-16|NEFT/SBIN11627014...|  null| 26-Sep-16|            |    1,000.00|    11,79,578.60|   2377| Deposit|\n",
      "|4998367691|29-Sep-16|NEFT/UTBIN1627373...|  null| 29-Sep-16|            |   21,000.00|    13,05,578.60|   2392| Deposit|\n",
      "|4998368406|24-Aug-16|RUPAY POS ACQ SET...|  null| 24-Aug-16|            |    3,357.09|-54,77,02,989.98|   3107| Deposit|\n",
      "|4998368550|23-Dec-16|RUPAY POS ACQ SET...|  null| 23-Dec-16|            | 9,08,269.11|-54,64,01,213.70|   3251| Deposit|\n",
      "|4998368568|03-Jan-17|VISA POS ACQ SETT...|  null| 03-Jan-17|            |10,75,484.03|-54,48,63,083.41|   3269| Deposit|\n",
      "|4998368652|30-Jan-17|RUPAY POS ACQ SET...|  null| 30-Jan-17|            | 5,71,987.24|-54,47,61,701.00|   3353| Deposit|\n",
      "|4998368692|15-Feb-17|RUPAY POS ACQ SET...|  null| 15-Feb-17|            | 6,48,182.82|-54,64,80,190.90|   3393| Deposit|\n",
      "|4998369092|08-Jun-17|MASTER POSDOM ACQ...|  null| 08-Jun-17|            | 2,81,884.39|-54,67,06,135.31|   3793| Deposit|\n",
      "|4998369259|11-Jul-17|RUPAY POS ACQ SET...|  null| 11-Jul-17|            |12,90,741.85|-54,45,80,132.48|   3960| Deposit|\n",
      "|4998369477|28-Aug-17|MASTER POSDOM ACQ...|  null| 28-Aug-17|            | 3,38,614.50|-54,64,93,169.91|   4178| Deposit|\n",
      "|4998369678|04-Oct-17|Indiaforensic MAS...|  null| 04-Oct-17|            | 2,57,720.13|-54,79,89,018.49|   4379| Deposit|\n",
      "|4998369843|03-Nov-17|Indiaforensic MAS...|  null| 03-Nov-17|            |        99.5|-54,40,17,375.70|   4544| Deposit|\n",
      "+----------+---------+--------------------+------+----------+------------+------------+----------------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Bank_log_details.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "abecb61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = r\"C:\\Users\\goelp\\OneDrive\\Desktop\\Output_files\\Bank_log_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c5cb4f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bank_log_details.write.mode('overwrite').parquet(os.path.join(output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "364d660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Schem = StructType(\n",
    "[\n",
    "  \n",
    "  StructField(\"Account_No\",LongType(),False),\n",
    "  StructField(\"Age\",IntegerType(),False),\n",
    "  StructField(\"Gender\",StringType(),False),\n",
    "  StructField(\"Education\",StringType(),False),\n",
    "  StructField(\"Marital_Status\",StringType(),False),\n",
    "  StructField(\"Acc_Type\",StringType(),False),\n",
    "  StructField(\"Months_On_Bank\",IntegerType(),False),\n",
    "  StructField(\"Credit_Card\",StringType(),False),\n",
    "  StructField(\"Credit_Score\",IntegerType(),False),\n",
    "  StructField(\"Estimated_Income\",StringType(),False),\n",
    "  StructField(\"Cust_id\",IntegerType(),False),\n",
    "  StructField(\"corrupt_record\",StringType(),True)  \n",
    "    \n",
    "]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9e573cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "            spark.read\n",
    "            .option(\"mode\",\"PERMISSIVE\")\n",
    "            .option(\"columnNameOfCorruptRecord\",\"corrupt_record\")\n",
    "            .csv(r\"C:\\Users\\goelp\\OneDrive\\Desktop\\archive\\Bank Customer Details.csv\",header = True,schema = Schem)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0822f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+-----------+--------------+--------+--------------+-----------+------------+----------------+-------+--------------+\n",
      "|Account_No|Age|Gender|  Education|Marital_Status|Acc_Type|Months_On_Bank|Credit_Card|Credit_Score|Estimated_Income|Cust_id|corrupt_record|\n",
      "+----------+---+------+-----------+--------------+--------+--------------+-----------+------------+----------------+-------+--------------+\n",
      "|4998365300| 45|  Male|High School|       Married| Savings|            42|        Yes|         619|       60K - 80K|      1|          null|\n",
      "|4998365301| 49|Female|   Graduate|        Single| Savings|            41|         No|         608|   Less than 40K|      2|          null|\n",
      "|4998365302| 51|  Male|   Graduate|       Married| Savings|            42|        Yes|         502|      80K - 120K|      3|          null|\n",
      "|4998365303| 40|Female|High School|          null| Savings|            39|         No|         699|   Less than 40K|      4|          null|\n",
      "|4998365304| 40|  Male| Uneducated|       Married| Savings|            43|        Yes|         850|       60K - 80K|      5|          null|\n",
      "+----------+---+------+-----------+--------------+--------+--------------+-----------+------------+----------------+-------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9bdbb82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Account_No: long (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Education: string (nullable = true)\n",
      " |-- Marital_Status: string (nullable = true)\n",
      " |-- Acc_Type: string (nullable = true)\n",
      " |-- Months_On_Bank: integer (nullable = true)\n",
      " |-- Credit_Card: string (nullable = true)\n",
      " |-- Credit_Score: integer (nullable = true)\n",
      " |-- Estimated_Income: string (nullable = true)\n",
      " |-- Cust_id: integer (nullable = true)\n",
      " |-- corrupt_record: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62c003e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+---------+--------------+--------+--------------+-----------+------------+----------------+-------+--------------------+\n",
      "|Account_No|Age|Gender|Education|Marital_Status|Acc_Type|Months_On_Bank|Credit_Card|Credit_Score|Estimated_Income|Cust_id|      corrupt_record|\n",
      "+----------+---+------+---------+--------------+--------+--------------+-----------+------------+----------------+-------+--------------------+\n",
      "|5998375299| 51|     1| Graduate|       Married| Savings|          null|        Yes|         792|      80K - 120K|  10000|5998375299,51,1,G...|\n",
      "|5998375299| 51|  Male|        0|       Married| Savings|            28|        Yes|         792|      80K - 120K|  10000|                null|\n",
      "+----------+---+------+---------+--------------+--------+--------------+-----------+------------+----------------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.Account_No == \"5998375299\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9d8e531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8cc1e0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+-------------+--------------+--------+--------------+-----------+------------+----------------+-------+--------------+\n",
      "|Account_No|Age|Gender|    Education|Marital_Status|Acc_Type|Months_On_Bank|Credit_Card|Credit_Score|Estimated_Income|Cust_id|corrupt_record|\n",
      "+----------+---+------+-------------+--------------+--------+--------------+-----------+------------+----------------+-------+--------------+\n",
      "|4998365300| 45|  Male|  High School|       Married| Savings|            42|        Yes|         619|       60K - 80K|      1|          null|\n",
      "|4998365301| 49|Female|     Graduate|        Single| Savings|            41|         No|         608|   Less than 40K|      2|          null|\n",
      "|4998365302| 51|  Male|     Graduate|       Married| Savings|            42|        Yes|         502|      80K - 120K|      3|          null|\n",
      "|4998365303| 40|Female|  High School|          null| Savings|            39|         No|         699|   Less than 40K|      4|          null|\n",
      "|4998365304| 40|  Male|   Uneducated|       Married| Savings|            43|        Yes|         850|       60K - 80K|      5|          null|\n",
      "|4998365305| 44|  Male|     Graduate|       Married| Savings|            44|        Yes|         645|       40K - 60K|      6|          null|\n",
      "|4998365306| 51|  Male|      Unknown|       Married|  Salary|            50|        Yes|         822|          120K +|      7|          null|\n",
      "|4998365307| 32|  Male|  High School|          null| Current|            29|        Yes|         376|       60K - 80K|      8|          null|\n",
      "|4998365308| 37|  Male|   Uneducated|        Single| Savings|            44|         No|         501|       60K - 80K|      9|          null|\n",
      "|4998365309| 48|  Male|     Graduate|        Single| Savings|            27|        Yes|         684|      80K - 120K|     10|          null|\n",
      "|4998365310| 42|  Male|   Uneducated|          null| Savings|            31|         No|         528|          120K +|     11|          null|\n",
      "|4998365311| 65|  Male|      Unknown|       Married| Savings|            24|        Yes|         497|       40K - 60K|     12|          null|\n",
      "|4998365312| 56|  Male|      College|        Single| Savings|            34|        Yes|         476|      80K - 120K|     13|          null|\n",
      "|4998365313| 35|  Male|     Graduate|          null| Savings|            25|         No|         549|       60K - 80K|     14|          null|\n",
      "|4998365314| 57|Female|     Graduate|       Married| Savings|            35|        Yes|         635|   Less than 40K|     15|          null|\n",
      "|4998365315| 44|  Male|      Unknown|          null| Savings|            45|         No|         616|      80K - 120K|     16|          null|\n",
      "|4998365316| 48|  Male|Post-Graduate|        Single| Savings|            58|        Yes|         653|      80K - 120K|     17|          null|\n",
      "|4998365317| 41|  Male|      Unknown|       Married| Savings|            24|        Yes|         549|      80K - 120K|     18|          null|\n",
      "|4998365318| 61|  Male|  High School|       Married| Savings|            45|         No|         587|       40K - 60K|     19|          null|\n",
      "|4998365319| 45|Female|     Graduate|       Married| Savings|            24|        Yes|         726|         Unknown|     20|          null|\n",
      "+----------+---+------+-------------+--------------+--------+--------------+-----------+------------+----------------+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9ec61f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Customer = df.select(\"Cust_id\",\n",
    "                      \"Gender\",\n",
    "                      \"Education\",\n",
    "                      \"Marital_Status\",\n",
    "                       \"Age\"\n",
    "                    ).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9fee0af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------------+--------------+----+\n",
      "|Cust_id|Gender|    Education|Marital_Status| Age|\n",
      "+-------+------+-------------+--------------+----+\n",
      "|     24|Female|      Unknown|        Single|null|\n",
      "|    261|  Male|      College|       Unknown|  36|\n",
      "|    462|  Male|      Unknown|        Single|  38|\n",
      "|    661|  Male|   Uneducated|       Unknown|null|\n",
      "|   1089|  Male|      College|       Married|  35|\n",
      "|   1320|  Male|     Graduate|       Married|  48|\n",
      "|   1562|Female|      College|       Married|  48|\n",
      "|   2136|  Male|  High School|        Single|  36|\n",
      "|   2155|  Male|      College|        Single|  26|\n",
      "|   2358|  Male|      College|       Married|  57|\n",
      "|   3065|  Male|     Graduate|       Married|  45|\n",
      "|   3108|Female|     Graduate|       Married|  43|\n",
      "|   3130|Female|      College|      Divorced|  53|\n",
      "|   3222|  Male|  High School|       Unknown|  35|\n",
      "|   3321|Female|      Unknown|       Married|  41|\n",
      "|   3534|  Male|   Uneducated|       Married|  44|\n",
      "|   3551|Female|  High School|        Single|  52|\n",
      "|   3819|  Male|    Doctorate|        Single|  50|\n",
      "|   3975|Female|Post-Graduate|       Married|  48|\n",
      "|   4254|Female|  High School|      Divorced|  50|\n",
      "+-------+------+-------------+--------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "339ab666",
   "metadata": {},
   "outputs": [],
   "source": [
    "Account = df.select(\"Account_No\",\n",
    "                    \"Acc_Type\",\n",
    "                    \"Months_On_Bank\",\n",
    "                    \"Credit_Score\",\n",
    "                    \"Credit_Card\",\n",
    "                    \"Estimated_Income\"\n",
    "                    ).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aae3f42c",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o189.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 28.0 failed 1 times, most recent failure: Lost task 0.0 in stage 28.0 (TID 42, LAPTOP-2743R0QM, executor driver): java.lang.IllegalArgumentException: corrupt_record does not exist. Available: Account_No, Age, Gender, Education, Marital_Status, Acc_Type, Months_On_Bank, Credit_Card, Credit_Score, Estimated_Income, Cust_id\r\n\tat org.apache.spark.sql.types.StructType.$anonfun$fieldIndex$1(StructType.scala:306)\r\n\tat scala.collection.MapLike.getOrElse(MapLike.scala:131)\r\n\tat scala.collection.MapLike.getOrElse$(MapLike.scala:129)\r\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:63)\r\n\tat org.apache.spark.sql.types.StructType.fieldIndex(StructType.scala:305)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$4(CSVFilters.scala:65)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$4$adapted(CSVFilters.scala:65)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$3(CSVFilters.scala:65)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$3$adapted(CSVFilters.scala:54)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.<init>(CSVFilters.scala:54)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.<init>(UnivocityParser.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$1(CSVFileFormat.scala:138)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:147)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:116)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\r\n\tat sun.reflect.GeneratedMethodAccessor79.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.IllegalArgumentException: corrupt_record does not exist. Available: Account_No, Age, Gender, Education, Marital_Status, Acc_Type, Months_On_Bank, Credit_Card, Credit_Score, Estimated_Income, Cust_id\r\n\tat org.apache.spark.sql.types.StructType.$anonfun$fieldIndex$1(StructType.scala:306)\r\n\tat scala.collection.MapLike.getOrElse(MapLike.scala:131)\r\n\tat scala.collection.MapLike.getOrElse$(MapLike.scala:129)\r\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:63)\r\n\tat org.apache.spark.sql.types.StructType.fieldIndex(StructType.scala:305)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$4(CSVFilters.scala:65)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$4$adapted(CSVFilters.scala:65)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$3(CSVFilters.scala:65)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$3$adapted(CSVFilters.scala:54)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.<init>(CSVFilters.scala:54)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.<init>(UnivocityParser.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$1(CSVFileFormat.scala:138)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:147)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:116)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2076/2888166443.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"corrupt_record\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misNotNull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark3\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    438\u001b[0m         \"\"\"\n\u001b[0;32m    439\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark3\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark3\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark3\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o189.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 28.0 failed 1 times, most recent failure: Lost task 0.0 in stage 28.0 (TID 42, LAPTOP-2743R0QM, executor driver): java.lang.IllegalArgumentException: corrupt_record does not exist. Available: Account_No, Age, Gender, Education, Marital_Status, Acc_Type, Months_On_Bank, Credit_Card, Credit_Score, Estimated_Income, Cust_id\r\n\tat org.apache.spark.sql.types.StructType.$anonfun$fieldIndex$1(StructType.scala:306)\r\n\tat scala.collection.MapLike.getOrElse(MapLike.scala:131)\r\n\tat scala.collection.MapLike.getOrElse$(MapLike.scala:129)\r\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:63)\r\n\tat org.apache.spark.sql.types.StructType.fieldIndex(StructType.scala:305)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$4(CSVFilters.scala:65)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$4$adapted(CSVFilters.scala:65)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$3(CSVFilters.scala:65)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$3$adapted(CSVFilters.scala:54)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.<init>(CSVFilters.scala:54)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.<init>(UnivocityParser.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$1(CSVFileFormat.scala:138)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:147)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:116)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\r\n\tat sun.reflect.GeneratedMethodAccessor79.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.IllegalArgumentException: corrupt_record does not exist. Available: Account_No, Age, Gender, Education, Marital_Status, Acc_Type, Months_On_Bank, Credit_Card, Credit_Score, Estimated_Income, Cust_id\r\n\tat org.apache.spark.sql.types.StructType.$anonfun$fieldIndex$1(StructType.scala:306)\r\n\tat scala.collection.MapLike.getOrElse(MapLike.scala:131)\r\n\tat scala.collection.MapLike.getOrElse$(MapLike.scala:129)\r\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:63)\r\n\tat org.apache.spark.sql.types.StructType.fieldIndex(StructType.scala:305)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$4(CSVFilters.scala:65)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$4$adapted(CSVFilters.scala:65)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$3(CSVFilters.scala:65)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$3$adapted(CSVFilters.scala:54)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.<init>(CSVFilters.scala:54)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.<init>(UnivocityParser.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$1(CSVFileFormat.scala:138)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:147)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:116)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"corrupt_record\").isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1349ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corrupt_records = df.select(\"corrupt_record\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
