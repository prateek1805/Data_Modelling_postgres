{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6421fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark\\\\spark3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3625b2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e417f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import col,lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5015da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,LongType,IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff0f4b89",
   "metadata": {},
   "outputs": [],
   "source": [
    " spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0451ee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(r\"C:\\Users\\goelp\\OneDrive\\Desktop\\archive\\bank.csv\",header = True,inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6026a9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+------+----------+------------+------------+------------+-------+\n",
      "|Account_No|     DATE|       TRANS_DETAILS|CHQ_NO|VALUE_DATE|WITHDRAW_AMT| DEPOSIT_AMT| BALANCE_AMT|Tras_id|\n",
      "+----------+---------+--------------------+------+----------+------------+------------+------------+-------+\n",
      "|4998365300|29-Jun-17|TRF FROM  Indiafo...|  null| 29-Jun-17|        null|10,00,000.00|10,00,000.00|      1|\n",
      "|4998365301|05-Jul-17|TRF FROM  Indiafo...|  null| 05-Jul-17|        null|10,00,000.00|20,00,000.00|      2|\n",
      "|4998365302|18-Jul-17|FDRL/INTERNAL FUN...|  null| 18-Jul-17|        null| 5,00,000.00|25,00,000.00|      3|\n",
      "|4998365303|01-Aug-17|TRF FRM  Indiafor...|  null| 01-Aug-17|        null|30,00,000.00|55,00,000.00|      4|\n",
      "|4998365304|16-Aug-17|FDRL/INTERNAL FUN...|  null| 16-Aug-17|        null| 5,00,000.00|60,00,000.00|      5|\n",
      "+----------+---------+--------------------+------+----------+------------+------------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9c23817",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.na.fill(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb86e450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+------+----------+------------+------------+------------+-------+\n",
      "|Account_No|     DATE|       TRANS_DETAILS|CHQ_NO|VALUE_DATE|WITHDRAW_AMT| DEPOSIT_AMT| BALANCE_AMT|Tras_id|\n",
      "+----------+---------+--------------------+------+----------+------------+------------+------------+-------+\n",
      "|4998365300|29-Jun-17|TRF FROM  Indiafo...|  null| 29-Jun-17|            |10,00,000.00|10,00,000.00|      1|\n",
      "|4998365301|05-Jul-17|TRF FROM  Indiafo...|  null| 05-Jul-17|            |10,00,000.00|20,00,000.00|      2|\n",
      "|4998365302|18-Jul-17|FDRL/INTERNAL FUN...|  null| 18-Jul-17|            | 5,00,000.00|25,00,000.00|      3|\n",
      "|4998365303|01-Aug-17|TRF FRM  Indiafor...|  null| 01-Aug-17|            |30,00,000.00|55,00,000.00|      4|\n",
      "|4998365304|16-Aug-17|FDRL/INTERNAL FUN...|  null| 16-Aug-17|            | 5,00,000.00|60,00,000.00|      5|\n",
      "|4998365305|16-Aug-17|FDRL/INTERNAL FUN...|  null| 16-Aug-17|            | 5,00,000.00|65,00,000.00|      6|\n",
      "|4998365306|16-Aug-17|FDRL/INTERNAL FUN...|  null| 16-Aug-17|            | 5,00,000.00|70,00,000.00|      7|\n",
      "|4998365307|16-Aug-17|FDRL/INTERNAL FUN...|  null| 16-Aug-17|            | 5,00,000.00|75,00,000.00|      8|\n",
      "|4998365308|16-Aug-17|FDRL/INTERNAL FUN...|  null| 16-Aug-17|            | 5,00,000.00|80,00,000.00|      9|\n",
      "|4998365309|16-Aug-17|FDRL/INTERNAL FUN...|  null| 16-Aug-17|            | 5,00,000.00|85,00,000.00|     10|\n",
      "|4998365310|16-Aug-17|INDO GIBL Indiafo...|  null| 16-Aug-17| 1,33,900.00|            |83,66,100.00|     11|\n",
      "|4998365311|16-Aug-17|INDO GIBL Indiafo...|  null| 16-Aug-17|   18,000.00|            |83,48,100.00|     12|\n",
      "|4998365312|16-Aug-17|INDO GIBL Indiafo...|  null| 16-Aug-17|    5,000.00|            |83,43,100.00|     13|\n",
      "|4998365313|16-Aug-17|INDO GIBL Indiafo...|  null| 16-Aug-17| 1,95,800.00|            |81,47,300.00|     14|\n",
      "|4998365314|16-Aug-17|INDO GIBL Indiafo...|  null| 16-Aug-17|   81,600.00|            |80,65,700.00|     15|\n",
      "|4998365315|16-Aug-17|INDO GIBL Indiafo...|  null| 16-Aug-17|   41,800.00|            |80,23,900.00|     16|\n",
      "|4998365316|16-Aug-17|INDO GIBL Indiafo...|  null| 16-Aug-17|   98,500.00|            |79,25,400.00|     17|\n",
      "|4998365317|16-Aug-17|INDO GIBL Indiafo...|  null| 16-Aug-17| 1,43,800.00|            |77,81,600.00|     18|\n",
      "|4998365318|16-Aug-17|INDO GIBL Indiafo...|  null| 16-Aug-17| 3,31,650.00|            |74,49,950.00|     19|\n",
      "|4998365319|16-Aug-17|INDO GIBL Indiafo...|  null| 16-Aug-17| 1,29,000.00|            |73,20,950.00|     20|\n",
      "+----------+---------+--------------------+------+----------+------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70edc704",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bank_log_details = df1.select(\"Account_No\",\n",
    "                              \"DATE\",\n",
    "                              \"TRANS_DETAILS\",\n",
    "                              \"CHQ_NO\",\n",
    "                              \"VALUE_DATE\",\n",
    "                              \"WITHDRAW_AMT\",\n",
    "                              \"DEPOSIT_AMT\",\n",
    "                              \"BALANCE_AMT\",\n",
    "                              \"Tras_id\").drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "861f9d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bank_log_details = Bank_log_details.withColumn(\"Action\", \\\n",
    "   when((Bank_log_details.WITHDRAW_AMT !=\"\"), lit(\"Withdraw\"))\\\n",
    "     .otherwise(lit(\"Deposit\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dfacbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+------+----------+------------+------------+----------------+-------+--------+\n",
      "|Account_No|     DATE|       TRANS_DETAILS|CHQ_NO|VALUE_DATE|WITHDRAW_AMT| DEPOSIT_AMT|     BALANCE_AMT|Tras_id|  Action|\n",
      "+----------+---------+--------------------+------+----------+------------+------------+----------------+-------+--------+\n",
      "|4998365327|16-Aug-17|INDO GIBL Indiafo...|  null| 16-Aug-17|   78,100.00|            |    62,01,467.00|     28|Withdraw|\n",
      "|4998365561|07-Feb-18|INDO GIBL Indiafo...|  null| 07-Feb-18|   61,500.00|            |    14,35,695.00|    262|Withdraw|\n",
      "|4998366033|29-Sep-18|Indfor INCOME IND...|  null| 29-Sep-18|    1,920.00|            |    10,28,562.00|    734|Withdraw|\n",
      "|4998366937|01-Mar-18|FDRL/INTERNAL FUN...|  null| 01-Mar-18|            | 3,00,000.00|    11,04,941.41|   1638| Deposit|\n",
      "|4998366986|09-Apr-18|FDRL/INTERNAL FUN...|  null| 09-Apr-18|            | 3,00,000.00|     8,80,613.78|   1687| Deposit|\n",
      "|4998367070|05-Jun-18|    BBPS DT 05062018|  null| 05-Jun-18|   78,982.56|            |     8,81,446.90|   1771|Withdraw|\n",
      "|4998367242|02-Nov-18|    BBPS DT 02112018|  null| 02-Nov-18|            | 3,55,637.06|    14,92,201.77|   1943| Deposit|\n",
      "|4998367549|30-Jul-16|NEFT/SBIN11621243...|  null| 30-Jul-16|            |    5,000.00|     6,47,578.60|   2250| Deposit|\n",
      "|4998367676|26-Sep-16|NEFT/SBIN11627014...|  null| 26-Sep-16|            |    1,000.00|    11,79,578.60|   2377| Deposit|\n",
      "|4998367691|29-Sep-16|NEFT/UTBIN1627373...|  null| 29-Sep-16|            |   21,000.00|    13,05,578.60|   2392| Deposit|\n",
      "|4998368406|24-Aug-16|RUPAY POS ACQ SET...|  null| 24-Aug-16|            |    3,357.09|-54,77,02,989.98|   3107| Deposit|\n",
      "|4998368550|23-Dec-16|RUPAY POS ACQ SET...|  null| 23-Dec-16|            | 9,08,269.11|-54,64,01,213.70|   3251| Deposit|\n",
      "|4998368568|03-Jan-17|VISA POS ACQ SETT...|  null| 03-Jan-17|            |10,75,484.03|-54,48,63,083.41|   3269| Deposit|\n",
      "|4998368652|30-Jan-17|RUPAY POS ACQ SET...|  null| 30-Jan-17|            | 5,71,987.24|-54,47,61,701.00|   3353| Deposit|\n",
      "|4998368692|15-Feb-17|RUPAY POS ACQ SET...|  null| 15-Feb-17|            | 6,48,182.82|-54,64,80,190.90|   3393| Deposit|\n",
      "|4998369092|08-Jun-17|MASTER POSDOM ACQ...|  null| 08-Jun-17|            | 2,81,884.39|-54,67,06,135.31|   3793| Deposit|\n",
      "|4998369259|11-Jul-17|RUPAY POS ACQ SET...|  null| 11-Jul-17|            |12,90,741.85|-54,45,80,132.48|   3960| Deposit|\n",
      "|4998369477|28-Aug-17|MASTER POSDOM ACQ...|  null| 28-Aug-17|            | 3,38,614.50|-54,64,93,169.91|   4178| Deposit|\n",
      "|4998369678|04-Oct-17|Indiaforensic MAS...|  null| 04-Oct-17|            | 2,57,720.13|-54,79,89,018.49|   4379| Deposit|\n",
      "|4998369843|03-Nov-17|Indiaforensic MAS...|  null| 03-Nov-17|            |        99.5|-54,40,17,375.70|   4544| Deposit|\n",
      "+----------+---------+--------------------+------+----------+------------+------------+----------------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Bank_log_details.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9cdd2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = r\"C:\\Users\\goelp\\OneDrive\\Desktop\\Output_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40787822",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bank_log_details.write.mode('overwrite').parquet(os.path.join(output_path+'Bank_log/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9bc8b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "Schem = StructType(\n",
    "[\n",
    "  \n",
    "  StructField(\"Account_No\",LongType(),False),\n",
    "  StructField(\"Age\",IntegerType(),False),\n",
    "  StructField(\"Gender\",StringType(),False),\n",
    "  StructField(\"Education\",StringType(),False),\n",
    "  StructField(\"Marital_Status\",StringType(),False),\n",
    "  StructField(\"Acc_Type\",StringType(),False),\n",
    "  StructField(\"Months_On_Bank\",IntegerType(),False),\n",
    "  StructField(\"Credit_Card\",StringType(),False),\n",
    "  StructField(\"Credit_Score\",IntegerType(),False),\n",
    "  StructField(\"Estimated_Income\",StringType(),False),\n",
    "  StructField(\"Cust_id\",IntegerType(),False),\n",
    "  StructField(\"corrupt_record\",StringType(),False)  \n",
    "    \n",
    "]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4a737cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bank_df = (\n",
    "            spark.read\n",
    "            .option(\"mode\",\"PERMISSIVE\")\n",
    "            .option(\"columnNameOfCorruptRecord\",\"corrupt_record\")\n",
    "            .csv(r\"C:\\Users\\goelp\\OneDrive\\Desktop\\archive\\Bank Customer Details.csv\",header = True,schema = Schem)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74192422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Account_No: long (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Education: string (nullable = true)\n",
      " |-- Marital_Status: string (nullable = true)\n",
      " |-- Acc_Type: string (nullable = true)\n",
      " |-- Months_On_Bank: integer (nullable = true)\n",
      " |-- Credit_Card: string (nullable = true)\n",
      " |-- Credit_Score: integer (nullable = true)\n",
      " |-- Estimated_Income: string (nullable = true)\n",
      " |-- Cust_id: integer (nullable = true)\n",
      " |-- corrupt_record: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Bank_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffb2eef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+-------------+--------------+--------+--------------+-----------+------------+----------------+-------+--------------+\n",
      "|Account_No|Age|Gender|    Education|Marital_Status|Acc_Type|Months_On_Bank|Credit_Card|Credit_Score|Estimated_Income|Cust_id|corrupt_record|\n",
      "+----------+---+------+-------------+--------------+--------+--------------+-----------+------------+----------------+-------+--------------+\n",
      "|4998365300| 45|  Male|  High School|       Married| Savings|            42|        Yes|         619|       60K - 80K|      1|          null|\n",
      "|4998365301| 49|Female|     Graduate|        Single| Savings|            41|         No|         608|   Less than 40K|      2|          null|\n",
      "|4998365302| 51|  Male|     Graduate|       Married| Savings|            42|        Yes|         502|      80K - 120K|      3|          null|\n",
      "|4998365303| 40|Female|  High School|          null| Savings|            39|         No|         699|   Less than 40K|      4|          null|\n",
      "|4998365304| 40|  Male|   Uneducated|       Married| Savings|            43|        Yes|         850|       60K - 80K|      5|          null|\n",
      "|4998365305| 44|  Male|     Graduate|       Married| Savings|            44|        Yes|         645|       40K - 60K|      6|          null|\n",
      "|4998365306| 51|  Male|      Unknown|       Married|  Salary|            50|        Yes|         822|          120K +|      7|          null|\n",
      "|4998365307| 32|  Male|  High School|          null| Current|            29|        Yes|         376|       60K - 80K|      8|          null|\n",
      "|4998365308| 37|  Male|   Uneducated|        Single| Savings|            44|         No|         501|       60K - 80K|      9|          null|\n",
      "|4998365309| 48|  Male|     Graduate|        Single| Savings|            27|        Yes|         684|      80K - 120K|     10|          null|\n",
      "|4998365310| 42|  Male|   Uneducated|          null| Savings|            31|         No|         528|          120K +|     11|          null|\n",
      "|4998365311| 65|  Male|      Unknown|       Married| Savings|            24|        Yes|         497|       40K - 60K|     12|          null|\n",
      "|4998365312| 56|  Male|      College|        Single| Savings|            34|        Yes|         476|      80K - 120K|     13|          null|\n",
      "|4998365313| 35|  Male|     Graduate|          null| Savings|            25|         No|         549|       60K - 80K|     14|          null|\n",
      "|4998365314| 57|Female|     Graduate|       Married| Savings|            35|        Yes|         635|   Less than 40K|     15|          null|\n",
      "|4998365315| 44|  Male|      Unknown|          null| Savings|            45|         No|         616|      80K - 120K|     16|          null|\n",
      "|4998365316| 48|  Male|Post-Graduate|        Single| Savings|            58|        Yes|         653|      80K - 120K|     17|          null|\n",
      "|4998365317| 41|  Male|      Unknown|       Married| Savings|            24|        Yes|         549|      80K - 120K|     18|          null|\n",
      "|4998365318| 61|  Male|  High School|       Married| Savings|            45|         No|         587|       40K - 60K|     19|          null|\n",
      "|4998365319| 45|Female|     Graduate|       Married| Savings|            24|        Yes|         726|         Unknown|     20|          null|\n",
      "+----------+---+------+-------------+--------------+--------+--------------+-----------+------------+----------------+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Bank_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d685628",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o75.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 229, LAPTOP-2743R0QM, executor driver): java.lang.IllegalArgumentException: corrupt_record does not exist. Available: Account_No, Age, Gender, Education, Marital_Status, Acc_Type, Months_On_Bank, Credit_Card, Credit_Score, Estimated_Income, Cust_id\r\n\tat org.apache.spark.sql.types.StructType.$anonfun$fieldIndex$1(StructType.scala:306)\r\n\tat scala.collection.MapLike.getOrElse(MapLike.scala:131)\r\n\tat scala.collection.MapLike.getOrElse$(MapLike.scala:129)\r\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:63)\r\n\tat org.apache.spark.sql.types.StructType.fieldIndex(StructType.scala:305)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$4(CSVFilters.scala:65)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$4$adapted(CSVFilters.scala:65)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$3(CSVFilters.scala:65)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$3$adapted(CSVFilters.scala:54)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.<init>(CSVFilters.scala:54)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.<init>(UnivocityParser.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$1(CSVFileFormat.scala:138)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:147)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:116)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.IllegalArgumentException: corrupt_record does not exist. Available: Account_No, Age, Gender, Education, Marital_Status, Acc_Type, Months_On_Bank, Credit_Card, Credit_Score, Estimated_Income, Cust_id\r\n\tat org.apache.spark.sql.types.StructType.$anonfun$fieldIndex$1(StructType.scala:306)\r\n\tat scala.collection.MapLike.getOrElse(MapLike.scala:131)\r\n\tat scala.collection.MapLike.getOrElse$(MapLike.scala:129)\r\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:63)\r\n\tat org.apache.spark.sql.types.StructType.fieldIndex(StructType.scala:305)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$4(CSVFilters.scala:65)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$4$adapted(CSVFilters.scala:65)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$3(CSVFilters.scala:65)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$3$adapted(CSVFilters.scala:54)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.<init>(CSVFilters.scala:54)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.<init>(UnivocityParser.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$1(CSVFileFormat.scala:138)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:147)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:116)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3820/3519027240.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mBank_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"corrupt_record\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misNotNull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark3\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    438\u001b[0m         \"\"\"\n\u001b[0;32m    439\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark3\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark3\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark3\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o75.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 229, LAPTOP-2743R0QM, executor driver): java.lang.IllegalArgumentException: corrupt_record does not exist. Available: Account_No, Age, Gender, Education, Marital_Status, Acc_Type, Months_On_Bank, Credit_Card, Credit_Score, Estimated_Income, Cust_id\r\n\tat org.apache.spark.sql.types.StructType.$anonfun$fieldIndex$1(StructType.scala:306)\r\n\tat scala.collection.MapLike.getOrElse(MapLike.scala:131)\r\n\tat scala.collection.MapLike.getOrElse$(MapLike.scala:129)\r\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:63)\r\n\tat org.apache.spark.sql.types.StructType.fieldIndex(StructType.scala:305)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$4(CSVFilters.scala:65)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$4$adapted(CSVFilters.scala:65)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$3(CSVFilters.scala:65)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$3$adapted(CSVFilters.scala:54)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.<init>(CSVFilters.scala:54)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.<init>(UnivocityParser.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$1(CSVFileFormat.scala:138)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:147)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:116)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.IllegalArgumentException: corrupt_record does not exist. Available: Account_No, Age, Gender, Education, Marital_Status, Acc_Type, Months_On_Bank, Credit_Card, Credit_Score, Estimated_Income, Cust_id\r\n\tat org.apache.spark.sql.types.StructType.$anonfun$fieldIndex$1(StructType.scala:306)\r\n\tat scala.collection.MapLike.getOrElse(MapLike.scala:131)\r\n\tat scala.collection.MapLike.getOrElse$(MapLike.scala:129)\r\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:63)\r\n\tat org.apache.spark.sql.types.StructType.fieldIndex(StructType.scala:305)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$4(CSVFilters.scala:65)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$4$adapted(CSVFilters.scala:65)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$3(CSVFilters.scala:65)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.$anonfun$predicates$3$adapted(CSVFilters.scala:54)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVFilters.<init>(CSVFilters.scala:54)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.<init>(UnivocityParser.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$1(CSVFileFormat.scala:138)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:147)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:116)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "Bank_df.filter(col(\"corrupt_record\").isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9603812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Customer = Bank_df.select(\"Cust_id\",\n",
    "                      \"Gender\",\n",
    "                      \"Education\",\n",
    "                      \"Marital_Status\",\n",
    "                       \"Age\",\n",
    "                       \"Account_No\"\n",
    "                    ).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b793003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------------+--------------+---+----------+\n",
      "|Cust_id|Gender|    Education|Marital_Status|Age|Account_No|\n",
      "+-------+------+-------------+--------------+---+----------+\n",
      "|     91|Female|  High School|        Single| 51|4998365390|\n",
      "|    331|Female|     Graduate|        Single| 43|4998365630|\n",
      "|    465|  Male|     Graduate|        Single| 50|4998365764|\n",
      "|    635|  Male|   Uneducated|       Married| 65|4998365934|\n",
      "|    775|  Male|      College|        Single| 46|4998366074|\n",
      "|    838|  Male|      College|       Married| 56|4998366137|\n",
      "|    909|Female|  High School|        Single| 46|4998366208|\n",
      "|   1080|Female|     Graduate|       Married| 45|4998366379|\n",
      "|   1330|  Male|     Graduate|       Married| 33|4998366629|\n",
      "|   1447|  Male|   Uneducated|      Divorced| 54|4998366746|\n",
      "|   1532|  Male|      Unknown|       Married| 39|4998366831|\n",
      "|   1582|Female|  High School|        Single| 33|4998366881|\n",
      "|   2275|Female|     Graduate|       Married| 61|4998367574|\n",
      "|   2370|  Male|      Unknown|       Married| 56|4998367669|\n",
      "|   2831|Female|   Uneducated|        Single| 46|4998368130|\n",
      "|   3043|  Male|     Graduate|        Single| 44|4998368342|\n",
      "|   3821|Female|Post-Graduate|        Single| 42|4998369120|\n",
      "|   3964|Female|   Uneducated|        Single| 41|4998369263|\n",
      "|   4005|  Male|   Uneducated|      Divorced| 42|4998369304|\n",
      "|   4136|  Male|      College|       Married| 44|4998369435|\n",
      "+-------+------+-------------+--------------+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "249507ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Account = Bank_df.select(\"Account_No\",\n",
    "                    \"Acc_Type\",\n",
    "                    \"Months_On_Bank\",\n",
    "                    \"Credit_Score\",\n",
    "                    \"Credit_Card\",\n",
    "                    \"Estimated_Income\"\n",
    "                    ).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b37421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_details = spark.read.parquet(os.path.join(output_path+'Bank_log/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce49911b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+------+----------+------------+--------------+----------------+-------+--------+\n",
      "|Account_No|     DATE|       TRANS_DETAILS|CHQ_NO|VALUE_DATE|WITHDRAW_AMT|   DEPOSIT_AMT|     BALANCE_AMT|Tras_id|  Action|\n",
      "+----------+---------+--------------------+------+----------+------------+--------------+----------------+-------+--------+\n",
      "|4998365368|25-Sep-17|INDO GIBL Indiafo...|  null| 25-Sep-17|   10,000.00|              |    21,63,202.00|     69|Withdraw|\n",
      "|4998365384|25-Sep-17|FDRL/INTERNAL FUN...|  null| 25-Sep-17|            |   5,00,000.00|    12,43,588.00|     85| Deposit|\n",
      "|4998366702|22-Aug-17|TRF FROM  Indiafo...|  null| 22-Aug-17|            |   3,00,000.00|    10,93,301.39|   1403| Deposit|\n",
      "|4998366704|24-Aug-17|BBPS SETMNT FOR D...|  null| 24-Aug-17| 2,38,134.06|              |     6,58,543.43|   1405|Withdraw|\n",
      "|4998366730|19-Sep-17|BBPS SETMNT FOR D...|  null| 19-Sep-17| 2,09,833.19|              |     8,68,227.69|   1431|Withdraw|\n",
      "|4998366833|05-Dec-17|FDRL/INTERNAL FUN...|  null| 05-Dec-17|            |   3,00,000.00|    13,47,410.67|   1534| Deposit|\n",
      "|4998366879|08-Jan-18|BBPS SETMNT FOR D...|  null| 08-Jan-18| 1,74,434.50|              |    10,65,931.57|   1580|Withdraw|\n",
      "|4998366951|12-Mar-18|FDRL/INTERNAL FUN...|  null| 12-Mar-18|            |   3,00,000.00|    11,28,153.38|   1652| Deposit|\n",
      "|4998367164|31-Aug-18|    BBPS DT 31082018|  null| 31-Aug-18|            |      8,238.05|    17,10,709.33|   1865| Deposit|\n",
      "|4998367416|20-Feb-19|    BBPS DT 20022019|  null| 20-Feb-19| 1,45,202.42|              |     5,69,295.80|   2117|Withdraw|\n",
      "|4998367971|23-Nov-16|NEFT/BKIDN1632851...|  null| 23-Nov-16|            |      8,000.00|    17,65,580.85|   2672| Deposit|\n",
      "|4998368316|30-Mar-16|RTGS/YESBH1606493...|  null| 30-Mar-16|            |1,50,00,000.00|-36,80,17,573.17|   3017| Deposit|\n",
      "|4998368414|01-Sep-16|RUPAY POS ACQ SET...|  null| 01-Sep-16|            |     17,498.32|-54,76,13,475.25|   3115| Deposit|\n",
      "|4998368548|23-Dec-16|VISA POS ACQ SETT...|  null| 23-Dec-16|            |   6,33,697.23|-54,43,09,482.81|   3249| Deposit|\n",
      "|4998368643|27-Jan-17|RUPAY POS ACQ SET...|  null| 27-Jan-17|            |   8,37,130.17|-54,57,87,413.78|   3344| Deposit|\n",
      "|4998368904|28-Apr-17|MASTER POSDOM ACQ...|  null| 28-Apr-17|            |   1,39,713.84|-54,68,75,722.97|   3605| Deposit|\n",
      "|4998368970|15-May-17|MASTER POSDOM ACQ...|  null| 15-May-17|            |   4,23,191.32|-54,47,31,176.55|   3671| Deposit|\n",
      "|4998369053|01-Jun-17|MAESTRO POSDOM AC...|  null| 01-Jun-17|            |   1,23,866.03|-54,54,68,312.02|   3754| Deposit|\n",
      "|4998369492|30-Aug-17|MAESTRO POSDOM AC...|  null| 30-Aug-17|            |     76,966.75|-54,77,92,920.98|   4193| Deposit|\n",
      "|4998369525|05-Sep-17|MAESTRO POSDOM AC...|  null| 05-Sep-17|            |     87,492.42|-54,43,24,714.30|   4226| Deposit|\n",
      "+----------+---------+--------------------+------+----------+------------+--------------+----------------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_details.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e95ac55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0667a96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bank_trans = Bank_df.join(log_details, Bank_df.Account_No == log_details.Account_No, how='inner')\\\n",
    "                        .select(monotonically_increasing_id().alias(\"Trans_id\"),\\\n",
    "                        col(\"Cust_id\"),\\\n",
    "                        Bank_df.Account_No,\\\n",
    "                        col(\"Action\"),\\\n",
    "                        col(\"DATE\"),\n",
    "                        col(\"VALUE_DATE\"),\\\n",
    "                        \"Acc_Type\"\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40d041b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------+--------+---------+----------+--------+\n",
      "|Trans_id|Cust_id|Account_No|  Action|     DATE|VALUE_DATE|Acc_Type|\n",
      "+--------+-------+----------+--------+---------+----------+--------+\n",
      "|       0|     69|4998365368|Withdraw|25-Sep-17| 25-Sep-17| Savings|\n",
      "|       1|     85|4998365384| Deposit|25-Sep-17| 25-Sep-17| Savings|\n",
      "|       2|   1403|4998366702| Deposit|22-Aug-17| 22-Aug-17| Savings|\n",
      "|       3|   1405|4998366704|Withdraw|24-Aug-17| 24-Aug-17| Savings|\n",
      "|       4|   1431|4998366730|Withdraw|19-Sep-17| 19-Sep-17| Savings|\n",
      "|       5|   1534|4998366833| Deposit|05-Dec-17| 05-Dec-17| Savings|\n",
      "|       6|   1580|4998366879|Withdraw|08-Jan-18| 08-Jan-18| Savings|\n",
      "|       7|   1652|4998366951| Deposit|12-Mar-18| 12-Mar-18| Savings|\n",
      "|       8|   1865|4998367164| Deposit|31-Aug-18| 31-Aug-18| Savings|\n",
      "|       9|   2117|4998367416|Withdraw|20-Feb-19| 20-Feb-19| Savings|\n",
      "|      10|   2672|4998367971| Deposit|23-Nov-16| 23-Nov-16| Savings|\n",
      "|      11|   3017|4998368316| Deposit|30-Mar-16| 30-Mar-16| Savings|\n",
      "|      12|   3115|4998368414| Deposit|01-Sep-16| 01-Sep-16| Savings|\n",
      "|      13|   3249|4998368548| Deposit|23-Dec-16| 23-Dec-16| Savings|\n",
      "|      14|   3344|4998368643| Deposit|27-Jan-17| 27-Jan-17| Savings|\n",
      "|      15|   3605|4998368904| Deposit|28-Apr-17| 28-Apr-17| Savings|\n",
      "|      16|   3671|4998368970| Deposit|15-May-17| 15-May-17| Savings|\n",
      "|      17|   3754|4998369053| Deposit|01-Jun-17| 01-Jun-17| Savings|\n",
      "|      18|   4193|4998369492| Deposit|30-Aug-17| 30-Aug-17| Savings|\n",
      "|      19|   4226|4998369525| Deposit|05-Sep-17| 05-Sep-17| Savings|\n",
      "+--------+-------+----------+--------+---------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Bank_trans.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "797a56db",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bank_trans.write.mode('overwrite').parquet(output_path+'Bank_trans/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2506a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "Customer.write.mode('overwrite').parquet(output_path+'Customer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8bc360fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Account.write.mode('overwrite').parquet(output_path+'Account/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a46801c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87f9379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556b9626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
