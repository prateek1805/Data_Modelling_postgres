                                                       Sparkify
Introduction

	A music streaming startup, Sparkify which want  to analyze the data they've been collecting on songs and user activity on their new music streaming app.
In this we have done data modeling with Postgres and build an ETL pipeline using Python. To complete the project, we were  needed to define fact and dimension tables for a star schema
In this project we have created ETL pipeline using python that extracts the data stored in json files and store it in panda dataframe and then moving the data to database, and transforms data into a set of dimensional tables.

Project Datasets:

We have worked with two datasets that reside in json format. The path  for each:

Song Dataset:
		The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.
For eg : song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json

Log dataset:
	The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above.
For eg: log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json


Schema for sparkify App

Fact Table
    1.	songplays - records in event data associated with song plays i.e. records with page NextSong
â€¢	songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
Dimension Tables
    1.	users - users in the app
        a.	user_id, first_name, last_name, gender, level
        
    2.	songs - songs in music database
        a.	song_id, title, artist_id, year, duration
        
    3.	artists - artists in music database
        a.	artist_id, name, location, lattitude, longitude
        
    4.	time - timestamps of records in songplays broken down into specific units
        a.	start_time, hour, day, week, month, year, weekday


Project Template:
The project template includes Five files excluding this file:

Create_table.py :
   In this file we have created the  fact and dimension tables for the star schema in Redshift.

    In this File we are creating the tables for redshift cluster. We import the sql_queries file and the  execute them one by one.

    1.	First we create the database with:

2.	Firstly there are SQL drop statement to drop the  table if any of them is present 

3.	Then there is function create_tables() that is used to create the staging, fact and dimension tables.

4.	This way we can run this file whenever we want to reset our database and test our ETL pipeline.

etl.py : This file is used to load data from json files to panda dataframe and then process that data into the analytics tables.
1.	In this we Implement the logic in to load data from json files to panda dataframe.
2.	After that we Implement the logic in etl.py to load data from Panda to analytics tables.

3.	The  etl.py is tested after running create_tables.py and running the analytic queries on your database to import the data from files to tables in database.

Sql_queries.py : This file contains all the SQL statements, which will be imported into the two other files above.


etl.ipynb : Reads and processes a single file from song_data and log_data and loads the data into our tables. This notebook contains detailed instructions on the ETL process for each of the tables. 

test.ipynb : It displays the first few rows of each table to let you check your database.

Create_table_test_ipynb : In this file we have created our database and tested the create_table file.
Firstly the tables are droped and then they are created.










